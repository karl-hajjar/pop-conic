{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc61dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38725de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93c8b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "NOTEBOOK_DIR = os.path.dirname(cwd+'/')\n",
    "ROOT = os.path.dirname(NOTEBOOK_DIR)\n",
    "\n",
    "import sys\n",
    "sys.path.append(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0251e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e0d9c",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0839a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 2 #50\n",
    "BIAS = False\n",
    "\n",
    "SEED = 42\n",
    "N_TRAIN = 256\n",
    "BATCH_SIZE = 64\n",
    "ALPHA = 1e-1\n",
    "BASE_LR = 1.0e-2\n",
    "N_STEPS = int(1.5e3)\n",
    "N_VAL = 100\n",
    "VAL_ITER = 100\n",
    "N_MAX_NEURONS = 2000\n",
    "\n",
    "EPS = 1.0e-7\n",
    "ALPHA = 0.1\n",
    "BETA = 1.0\n",
    "LAMBDA = 0.0 #0.01\n",
    "TAU = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de4bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURES_DIR = os.path.join(ROOT, 'figures/kernel_penalty_variants/')\n",
    "create_dir(FIGURES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50d3a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5073e61",
   "metadata": {},
   "source": [
    "## Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc00fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(a, b, x):\n",
    "    m = a.shape[1]\n",
    "    if m == 0:\n",
    "        return 0.\n",
    "    return np.matmul(np.maximum(np.matmul(x, b.T), 0), a.T) #/ a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8062c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(c, u, kappa, sigma=1.0):\n",
    "    if kappa == 'kappa_1':\n",
    "        kappa = kappa_1\n",
    "    elif kappa == 'kappa_2':\n",
    "        kappa = kappa_2\n",
    "    elif kappa == 'kappa_3':\n",
    "        kappa = kappa_3\n",
    "    else:\n",
    "        kappa = kappa_1\n",
    "    gram_mat = np.matmul(u, u.T)\n",
    "    K = kappa(gram_mat, sigma=sigma)\n",
    "    return 0.5 * np.matmul(np.abs(c), np.matmul(K, np.abs(c.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f280b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTRACTIVE\n",
    "def kappa_1(s, sigma=1.0):\n",
    "    return 1 - np.exp((s - 1) / sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8123013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPULSIVE\n",
    "def kappa_3(s, sigma=1.0):\n",
    "    return np.exp((s - 1) / sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b8a8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_conic_update(x, y, c, u, kappa, sigma=1.0, penalty=True):\n",
    "    if kappa == 'kappa_1':\n",
    "        grad_sign = -1.0\n",
    "        kappa = kappa_1\n",
    "    elif kappa == 'kappa_3':\n",
    "        grad_sign = 1.0\n",
    "        kappa = kappa_3\n",
    "    else:\n",
    "        grad_sign = 1.0\n",
    "    signs = np.sign(c)    \n",
    "    y_hat = forward(c, u, x)\n",
    "    n = x.shape[0]\n",
    "    \n",
    "    gram_mat = np.matmul(u, u.T)\n",
    "    K = kappa(gram_mat, sigma=sigma)\n",
    "    \n",
    "    # mass updates\n",
    "    grad_H = np.matmul(np.abs(c), K)\n",
    "    grad_J = np.matmul((y_hat - y).T, np.maximum(np.matmul(x, u.T), 0)) / n\n",
    "    grad = TAU * signs * grad_J # signs for the data-fitting term\n",
    "    if penalty:\n",
    "        #grad += LAMBDA * grad_H\n",
    "        pass\n",
    "    grad_norm = np.mean(np.linalg.norm(grad, axis=1, ord=2, keepdims=True))\n",
    "    grad = (grad / grad_norm) * np.minimum(grad_norm, 0.001)\n",
    "    c = (1 - 2 * ETA * grad) * c  # no sign here since only the absolute values of the masses interact in the penalty\n",
    "    \n",
    "    # position updates\n",
    "    #grad_H = grad_sign * (1 / sigma**2) * (np.matmul(np.abs(c) * K, u) - np.matmul(K * gram_mat, np.abs(c).T) * u)\n",
    "    #grad_H = -(1 / sigma**2) * (np.matmul(c * K, u) - np.matmul(K * gram_mat, c.T) * u)\n",
    "    #grad_H = (1 / sigma**2) * (np.matmul(np.abs(c) * K, u) - np.matmul(K * gram_mat, np.abs(c.T)) * u)\n",
    "    # adjusted learning rate\n",
    "    grad_H = grad_sign * (np.matmul(np.abs(c) * K, u) - np.matmul(K * gram_mat, np.abs(c).T) * u)\n",
    "    grad_J = np.matmul(np.heaviside(np.matmul(u, x.T), 0), (y_hat - y)*x) / n\n",
    "    grad = TAU * signs.T * grad_J\n",
    "    \n",
    "    if penalty:\n",
    "        grad += LAMBDA * grad_H\n",
    "    #grad_norm = np.mean(np.linalg.norm(grad, axis=1, ord=2, keepdims=True))\n",
    "    #grad = (grad / grad_norm) * np.minimum(grad_norm, 0.001)\n",
    "    \n",
    "    u = u - ETA * grad\n",
    "    \n",
    "    # re-normalize particle positions on the sphere\n",
    "    u = u / np.linalg.norm(u, axis=1, ord=2, keepdims=True)\n",
    "    \n",
    "    return c, u, grad, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e2d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_neurons(c, u, eps=1e-4):            \n",
    "    squared_dists = 2 * (1 - np.matmul(u, u.T))  # vectors in u are of unit norm\n",
    "    squared_dists += np.identity(len(squared_dists))\n",
    "    dist_indexes_to_remove = np.argwhere(squared_dists < (eps**2))\n",
    "    \n",
    "    if len(dist_indexes_to_remove) > 0:\n",
    "        cpt = 0\n",
    "        i0, j = dist_indexes_to_remove[0]\n",
    "        to_merge = {i0 : []}\n",
    "        merged = set([i0])\n",
    "        for k in range(len(dist_indexes_to_remove)):\n",
    "            i, j = dist_indexes_to_remove[k]\n",
    "            if i == i0:\n",
    "                to_merge[i0].append(j)\n",
    "                merged.add(j)\n",
    "            else:\n",
    "                if i not in merged:\n",
    "                    i0 = i\n",
    "                    merged.add(i)\n",
    "                    to_merge[i] = []\n",
    "                    if j not in merged:\n",
    "                        to_merge[i].append(j)\n",
    "                        merged.add(i)\n",
    "                        merged.add(j)\n",
    "        \n",
    "        index_to_remove = []\n",
    "        for i, js in to_merge.items():\n",
    "            index_to_remove += js\n",
    "            if len(js) > 0:\n",
    "                c[0, i] += np.sum(c[0, js])\n",
    "\n",
    "        u = np.delete(u, index_to_remove, axis=0)\n",
    "        c = np.delete(c, index_to_remove, axis=1)\n",
    "    \n",
    "    return c, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92ca3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neuron(c, u, x, y, kappa, penalty=False, n_init=2):\n",
    "    new_u = np.random.normal(size=(1, INPUT_DIM))\n",
    "    new_u = new_u / np.linalg.norm(new_u, axis=1, ord=2, keepdims=True)\n",
    "\n",
    "    if kappa == 'kappa_1':\n",
    "        grad_sign = -1.0\n",
    "        kappa = kappa_1\n",
    "    elif kappa == 'kappa_3':\n",
    "        grad_sign = 1.0\n",
    "        kappa = kappa_3\n",
    "    else:\n",
    "        grad_sign = 1.0\n",
    "\n",
    "    n = x.shape[0]\n",
    "    if (c is None) or (u is None):\n",
    "        new_u = np.random.normal(size=(n_init, INPUT_DIM))\n",
    "        new_u = new_u / np.linalg.norm(new_u, axis=1, ord=2, keepdims=True)\n",
    "        \n",
    "        # mass update\n",
    "        y_hat = 0\n",
    "        grad_J = np.matmul((y_hat - y).T, np.maximum(np.matmul(x, new_u.T), 0)) / n\n",
    "        grad = TAU * grad_J\n",
    "        #grad = grad_J\n",
    "        \n",
    "        #c = - ALPHA * (grad + np.random.normal(size=grad.shape))\n",
    "        c = - ALPHA * grad\n",
    "        u = new_u\n",
    "    \n",
    "    else:    \n",
    "        new_u = np.random.normal(size=(1, INPUT_DIM))\n",
    "        new_u = new_u / np.linalg.norm(new_u, axis=1, ord=2, keepdims=True)\n",
    "        \n",
    "        y_hat = forward(c, u, x)\n",
    "        signs = np.sign(c)    \n",
    "        gram_mat = np.matmul(u, new_u.T)\n",
    "        K = kappa(gram_mat, sigma=sigma)\n",
    "\n",
    "        # mass updates\n",
    "        if penalty:\n",
    "            grad_H = np.matmul(np.abs(c), K)\n",
    "        else:\n",
    "            grad_H = 0\n",
    "        grad_J = np.matmul((y_hat - y).T, np.maximum(np.matmul(x, new_u.T), 0)) / n\n",
    "\n",
    "        grad = TAU * grad_J + LAMBDA * grad_H  # signs for the data-fitting term\n",
    "        \n",
    "        #grad_norm = np.mean(np.linalg.norm(grad, axis=1, ord=2, keepdims=True))\n",
    "        #grad = (grad / grad_norm) * np.minimum(grad_norm, 0.02)\n",
    "\n",
    "        new_c = - ALPHA * grad\n",
    "        c = np.hstack((c, new_c))\n",
    "        u = np.vstack((u, new_u))\n",
    "    return c, u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e13f5a",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c02145a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 10 # 10\n",
    "noise = 1e-5\n",
    "\n",
    "m_star = 50\n",
    "a_star = np.random.normal(size=(1, m_star))\n",
    "b_star = np.random.normal(size=(m_star, INPUT_DIM)) / np.sqrt(INPUT_DIM)\n",
    "\n",
    "n_train = 50\n",
    "x_train = np.random.normal(size=(n_train, INPUT_DIM))\n",
    "x_train = x_train / np.linalg.norm(x_train, axis=1, ord=2, keepdims=True)\n",
    "y_train = forward(a_star, b_star, x_train) / m_star + noise * np.random.normal(size=(n_train, 1))\n",
    "\n",
    "n_val = 50\n",
    "x_val = np.random.normal(size=(n_val, INPUT_DIM))\n",
    "x_val = x_val / np.linalg.norm(x_val, axis=1, ord=2, keepdims=True)\n",
    "y_val = forward(a_star, b_star, x_val) / m_star + noise * np.random.normal(size=(n_val, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286e5ce",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97022267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(c, u, kappa, sigma=1.0, eps=1e-4, conic=False, dynamic=True, n_updates=1, pop_penalty=False, pop=True, \n",
    "          penalty=True):\n",
    "    us = [u]\n",
    "    cs = [c]\n",
    "    m0 = c.shape[1]\n",
    "    ms = [c.shape[1]]\n",
    "    y_hat = forward(c, u, x_train)\n",
    "    y_hats = [y_hat]\n",
    "    \n",
    "    Ks = []\n",
    "    \n",
    "    loss = TAU * np.mean((y_hat - y_train)**2) /2\n",
    "    losses = [loss]\n",
    "    penalized_losses = [loss + LAMBDA * H(c, u, kappa, sigma)[0, 0]]\n",
    "    grads = []\n",
    "    \n",
    "    for k in range(N_STEPS // n_updates):\n",
    "        if pop:\n",
    "            if conic:\n",
    "                if c.shape[1] < N_MAX_NEURONS:\n",
    "                    if k % 2 == 0:\n",
    "                        c, u = add_neuron(c, u, x_train, y_train, kappa, penalty=pop_penalty)\n",
    "                    else:      \n",
    "                        for _ in range(n_updates):\n",
    "                            c, u, grad, K = projected_conic_update(x_train, y_train, c, u, kappa=kappa, sigma=sigma, \n",
    "                                                                penalty=penalty)\n",
    "                            grads.append(grad)\n",
    "                            Ks.append(K)\n",
    "                        if dynamic:\n",
    "                            c, u = remove_neurons(c, u, eps=eps)\n",
    "#                        if c.shape[1] < m0:\n",
    "#                            c, u = add_neuron(c, u, x_train, y_train, kappa, penalty)\n",
    "\n",
    "            else:\n",
    "                c, u = add_neuron(c, u, x_train, y_train, kappa, penalty=pop_penalty)\n",
    "                        \n",
    "        else:\n",
    "            for _ in range(n_updates):\n",
    "                c, u, grad = projected_conic_update(x_train, y_train, c, u, kappa=kappa, sigma=sigma, penalty=penalty)\n",
    "\n",
    "        \n",
    "            if dynamic:\n",
    "                c, u = remove_neurons(c, u, eps=eps)\n",
    "                if conic:\n",
    "                    if c.shape[1] < m0:\n",
    "                        c, u = add_neuron(c, u, x_train, y_train, kappa, penalty=pop_penalty)\n",
    "                    \n",
    "        ms.append(c.shape[1])\n",
    "        \n",
    "        y_hat = forward(c, u, x_train)\n",
    "        loss = TAU * np.mean((y_hat - y_train)**2) / 2\n",
    "        penalized_loss = loss + LAMBDA * H(c, u, kappa, sigma)[0, 0]\n",
    "        y_hats.append(y_hat)\n",
    "        \n",
    "        cs.append(c)\n",
    "        us.append(u)\n",
    "        losses.append(loss)\n",
    "        penalized_losses.append(penalized_loss)\n",
    "        \n",
    "    return cs, us, np.array(losses), np.array(penalized_losses), ms, y_hats, grads, Ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9827b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(c, u, conic=False):\n",
    "    y_hat = forward(c, u, x_val)\n",
    "    loss = TAU * np.mean((y_hat - y_val)**2) / 2\n",
    "    if conic:\n",
    "        penalized_loss = loss\n",
    "    else:\n",
    "        penalized_loss = loss + LAMBDA * H(c, u, kappa, sigma)[0, 0]\n",
    "    return loss, penalized_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccfe8ea",
   "metadata": {},
   "source": [
    "## 1. Usual pop-conic with and without penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPULSIVE makes more sense as the pop step does not increase the penalized loss\n",
    "N_MAX_NEURONS = 3000\n",
    "ETA = 0.01\n",
    "sigma = 0.5 # 0.3\n",
    "N_STEPS = 30000 #6000 #3000\n",
    "eps = 0.08 # 0.6\n",
    "n_updates = 10\n",
    "\n",
    "TAU = 1.0 #1.0\n",
    "ALPHA = 0.1 ## 1.0 ---> CRUCIAL !!\n",
    "LAMBDA = 0.01 # 1.0, 0.5 0.02\n",
    "kappa = 'kappa_1' # 'kappa_3'\n",
    "\n",
    "# POP-CONIC-PENALTY\n",
    "pop_penalty = True # False\n",
    "dynamic = True\n",
    "pop = True\n",
    "conic = True\n",
    "penalty = True\n",
    "n_init = 2\n",
    "c0_, u0_ = add_neuron(None, None, x_train, y_train, kappa, n_init=n_init)\n",
    "c, u, losses3, penalized_losses3, ms3, _, grads, Ks = \\\n",
    "        train(c0_, u0_, kappa=kappa, sigma=sigma, eps=eps, n_updates=n_updates, \n",
    "              conic=conic, dynamic=dynamic, pop_penalty=pop_penalty, pop=pop, penalty=penalty)\n",
    "#        train(c0, u0, kappa='kappa_1', sigma=sigma, eps=eps, dynamic=dynamic, n_updates=n_updates, penalty=penalty)\n",
    "\n",
    "# POP-CONIC\n",
    "#pop_penalty = False\n",
    "#dynamic = True\n",
    "#pop = True\n",
    "#conic = True\n",
    "#penalty = False\n",
    "#c, u, losses_pop, penalized_losses_pop, ms_pop, _ = \\\n",
    "#        train(c0_, u0_, kappa=kappa, sigma=sigma, eps=eps, n_updates=n_updates, \n",
    "#              conic=conic, dynamic=dynamic, pop_penalty=pop_penalty, pop=pop, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d198dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_norms = [np.mean(np.linalg.norm(grad_, axis=1, ord=2, keepdims=True)) for grad_ in grads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24887bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_ = u[-1]\n",
    "\n",
    "if kappa == 'kappa_1':\n",
    "    grad_sign = -1.0\n",
    "    kappa = kappa_1\n",
    "elif kappa == 'kappa_3':\n",
    "    grad_sign = 1.0\n",
    "    kappa = kappa_3\n",
    "else:\n",
    "    pass\n",
    "gram_mat = np.matmul(u_, u_.T)\n",
    "K = kappa(gram_mat, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b5252",
   "metadata": {},
   "outputs": [],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(K[K>0.9]) / K.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6019a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f5654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebf543",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "marker = None #'o'\n",
    "#plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.plot(np.arange(len(grad_norms)), grad_norms, label='pop-repuls')\n",
    "#plt.plot(np.arange(len(train_losses_mix_conic)), train_losses_mix_conic, label='mix-conic')\n",
    "\n",
    "plt.xlabel('Iteration $k$', fontsize=28)\n",
    "plt.ylabel('Grad norms', fontsize=28)\n",
    "#plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'losses-ista-10-1.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ccce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "marker = None #'o'\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "\n",
    "plt.plot(np.arange(len(losses3)), losses3, label='pop-repuls')\n",
    "#plt.plot(np.arange(len(losses_pop)), losses_pop, label='pop-conic')\n",
    "#plt.plot(np.arange(len(train_losses_mix_conic)), train_losses_mix_conic, label='mix-conic')\n",
    "\n",
    "plt.xlabel('Iteration $k$', fontsize=28)\n",
    "plt.ylabel('Fitting term $\\\\ J(\\\\mu_m(k))$', fontsize=28)\n",
    "#plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'losses-ista-10-1.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "marker = None #'o'\n",
    "#plt.xscale('log')\n",
    "\n",
    "#plt.yscale('log')\n",
    "\n",
    "k_min, k_max = 0, N_STEPS + 1\n",
    "\n",
    "plt.plot(np.arange(len(penalized_losses3))[k_min:k_max], penalized_losses3[k_min:k_max], label='pop-repuls')\n",
    "#plt.plot(np.arange(len(penalized_losses_pop)), penalized_losses_pop, label='pop-conic')\n",
    "#plt.plot(np.arange(len(penalized_train_losses_mix_coni c)), penalized_train_losses_mix_conic, label='mix-conic')\n",
    "\n",
    "plt.xlabel('Iteration $k$', fontsize=28)\n",
    "plt.ylabel('Objective $\\\\ F(\\\\mu_m(k))$', fontsize=28)\n",
    "#plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'losses-ista-10-1.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "penalties = penalized_losses3 - losses3\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "marker =  'o' # None\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "\n",
    "plt.plot(np.arange(len(penalties)), penalties, label='pop-attract', marker=marker)\n",
    "#plt.plot(np.arange(len(losses_pop)), losses_pop, label='pop-conic')\n",
    "#plt.plot(np.arange(len(train_losses_mix_conic)), train_losses_mix_conic, label='mix-conic')\n",
    "\n",
    "plt.xlabel('Iteration $k$', fontsize=28)\n",
    "plt.ylabel('Penalty $\\\\ H(\\\\mu_m(k))$', fontsize=28)\n",
    "#plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'losses-ista-10-1.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137cc9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "penalties = penalized_losses3 - losses3\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "marker =  'o' # None\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "\n",
    "plt.plot(np.arange(len(penalties))[:35], penalties[:35], label='pop-attract', marker=marker)\n",
    "#plt.plot(np.arange(len(losses_pop)), losses_pop, label='pop-conic')\n",
    "#plt.plot(np.arange(len(train_losses_mix_conic)), train_losses_mix_conic, label='mix-conic')\n",
    "\n",
    "plt.xlabel('Iteration $k$', fontsize=28)\n",
    "plt.ylabel('Penalty $\\\\ H(\\\\mu_m(k))$', fontsize=28)\n",
    "#plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'losses-ista-10-1.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1397bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MAYBE THE PROBLME COMES FROM THE ABSOLUTE VALUES IN THE KERNEL PENALTY <#####\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "marker = None #'o'\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "\n",
    "plt.plot(np.arange(len(penalized_losses3)), penalized_losses3, label='pop-attract')\n",
    "#plt.plot(np.arange(len(penalized_losses_pop)), penalized_losses_pop, label='pop-conic')\n",
    "#plt.plot(np.arange(len(penalized_train_losses_mix_conic)), penalized_train_losses_mix_conic, label='mix-conic')\n",
    "\n",
    "plt.xlabel('Iteration $k$', fontsize=28)\n",
    "plt.ylabel('Objective $\\\\ F(\\\\mu_m(k))$', fontsize=28)\n",
    "#plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'losses-ista-10-1.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e3987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MAYBE THE PROBLME COMES FROM THE ABSOLUTE VALUES IN THE KERNEL PENALTY <#####\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "marker = None #'o'\n",
    "#plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.plot(np.arange(len(penalized_losses3)), penalized_losses3, label='pop-attract')\n",
    "#plt.plot(np.arange(len(penalized_losses_pop)), penalized_losses_pop, label='pop-conic')\n",
    "#plt.plot(np.arange(len(penalized_train_losses_mix_conic)), penalized_train_losses_mix_conic, label='mix-conic')\n",
    "\n",
    "plt.xlabel('Iteration $k$', fontsize=28)\n",
    "plt.ylabel('Objective $\\\\ F(\\\\mu_m(k))$', fontsize=28)\n",
    "#plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'losses-ista-10-1.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "marker = None #'o'\n",
    "#plt.xscale('log')\n",
    "\n",
    "#plt.yscale('log')\n",
    "\n",
    "k_min, k_max = 0, 100\n",
    "\n",
    "plt.plot(np.arange(len(penalized_losses3))[k_min:k_max], penalized_losses3[k_min:k_max], label='pop-repuls')\n",
    "#plt.plot(np.arange(len(penalized_losses_pop)), penalized_losses_pop, label='pop-conic')\n",
    "#plt.plot(np.arange(len(penalized_train_losses_mix_coni c)), penalized_train_losses_mix_conic, label='mix-conic')\n",
    "\n",
    "plt.xlabel('Iteration $k$', fontsize=28)\n",
    "plt.ylabel('Objective $\\\\ F(\\\\mu_m(k))$', fontsize=28)\n",
    "#plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'losses-ista-10-1.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = []\n",
    "for i in range(len(ms3)-1):\n",
    "    if ms3[i+1] < ms3[i]:\n",
    "        indx.append(i)\n",
    "        \n",
    "indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb14a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "marker = None #'o'\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "\n",
    "plt.plot(np.arange(len(ms3)), ms3, label='pop-repuls')\n",
    "#plt.plot(np.arange(len(ms_pop)), ms_pop, label='pop-conic')\n",
    "\n",
    "#plt.plot(np.arange(len(ms_mix_conic)), ms_mix_conic, label='mix-conic')\n",
    "\n",
    "plt.xlabel('Iteration $k$', fontsize=28)\n",
    "plt.ylabel('Number of neurons $m$', fontsize=28)\n",
    "#plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'losses-ista-10-1.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7c848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
